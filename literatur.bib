@article{Schroeder2010,
abstract = {Designing highly dependable systems requires a good understanding of failure characteristics. Unfortunately, little raw data on failures in large IT installations are publicly available. This paper analyzes failure data collected at two large high-performance computing sites. The first data set has been collected over the past nine years at Los Alamos National Laboratory (LANL) and has recently been made publicly available. It covers 23,000 failures recorded on more than 20 different systems at LANL, mostly large clusters of SMP and NUMA nodes. The second data set has been collected over the period of one year on one large supercomputing system comprising 20 nodes and more than 10,000 processors. We study the statistics of the data, including the root cause of failures, the mean time between failures, and the mean time to repair. We find, for example, that average failure rates differ wildly across systems, ranging from 20-1000 failures per year, and that time between failures is modeled well by a Weibull distribution with decreasing hazard rate. From one system to another, mean repair time varies from less than an hour to more than a day, and repair times are well modeled by a lognormal distribution.},
author = {Schroeder, Bianca and Gibson, Garth a},
doi = {10.1109/TDSC.2009.4},
file = {:media/Daten/Studium/10-Semester/Analyse eines Forschungthemas/Paper/A Large-Scale Study of Failures.pdf:pdf},
isbn = {1545-5971 VO - 7},
issn = {1545-5971},
journal = {IEEE Trans. Dependable Secur. Comput.},
mendeley-groups = {ForschProj},
number = {4},
pages = {337--350},
title = {{A Large-Scale Study of Failures in High-Performance Computing Systems}},
volume = {7},
year = {2010}
}

%Tools
@article{Kufrin2005,
abstract = {The motivation, design, implementation, and current status of a new set of software tools called PerfSuite that is targeted to performance analysis of user applications on Linux-based systems is described. These tools emphasize ease of use/deployment and portability/reuse in implementation details as well as data representation and format. After a year of public beta availability and production deployment on Linux clusters that rank among the largest-scale in the country, PerfSuite is gaining acceptance as a user-oriented and flexible software tool set that is as valuable on the desktop as it is on leading-edge terascale clusters.},
author = {Kufrin, Rick},
file = {:home/tinker/Studium/10-Semester/Analyse eines Forschungthemas/Paper/perfsuite.pdf:pdf},
journal = {Dans Present. 6th Int. Conf. Linux Clust. HPC Revolut.},
mendeley-groups = {ForschProj},
number = {April},
pages = {5},
title = {{Perfsuite: An accessible, open source performance analysis environment for linux}},
volume = {151},
year = {2005}
}

@article{Mucci2005,
abstract = {We present PerfMiner, a system for the transparent collection, storage and presentation of thread-level hardware performance data across an entire cluster. Every sub-process/thread spawned by the user through the batch system is measured with near zero overhead and no dilation of run-time. Performance metrics are collected at the thread level using tool built on top of the Performance Application Programming Interface (PAPI). As the hardware counters are virtualized by the OS, the resulting counts are largely unaffected by other kernel or user processes. PerfMiner correlates this performance data with metadata from the batch system and places it in a database. Through a command line and web interface, the user can make queries to the database to report information on everything from overall workload characterization and system utilization to the performance of a single thread in a specific application. This is in contrast to other monitoring systems that report aggregate system-wide metrics sampled over a period of time. In this paper, we describe our implementation of PerfMiner as well as present some results from the test deployment of PerfMiner across three different clusters at the Center for Parallel Computers at The Royal Institute of Technology in Stockholm, Sweden.},
author = {Mucci, P J and Ahlin, D and Danielsson, J and Ekman, P and Malinowski, L},
file = {:home/tinker/Studium/10-Semester/Analyse eines Forschungthemas/Paper/perfminer.pdf:pdf},
isbn = {0302-9743},
issn = {03029743},
journal = {Euro-Par 2005 Parallel Process. Proc.},
mendeley-groups = {ForschProj},
pages = {124--133},
title = {{PerfMiner: Cluster-wide collection, storage and presentation of application level hardware performance data}},
volume = {3648},
year = {2005}
}

@article{Kerbyson,
author = {Kerbyson, Darren and Rajamony, Ram and Hensbergen, Eric Van},
file = {:home/tinker/Studium/10-Semester/Analyse eines Forschungthemas/Paper/PHM-whist-2012-kerbyson.pdf:pdf},
mendeley-groups = {ForschProj},
title = {{Performance Health Monitoring for Large-Scale Systems}}
}

@inproceedings{Sottile2002,
abstract = {Supermon is a flexible set of tools for high speed, scalable cluster monitoring. Node behavior can be monitored much faster than with other commonly used methods (e.g., rstatd). In addition, Supermon uses a data protocol based on symbolic expressions (S-expressions) at all levels of Supermon, from individual nodes to entire clusters. This contributes to Supermon's scalability and allows it to function in a heterogeneous environment. This paper presents the Supermon architecture and discuss initial performance measurements on a cluster of heterogeneous Alpha-processor based nodes.},
author = {Sottile, M. J. and Minnich, R. G.},
booktitle = {Proc. - IEEE Int. Conf. Clust. Comput. ICCC},
doi = {10.1109/CLUSTR.2002.1137727},
file = {:home/tinker/Studium/10-Semester/Analyse eines Forschungthemas/Paper/supermon.pdf:pdf},
isbn = {0769517455},
issn = {15525244},
keywords = {Monitoring},
mendeley-groups = {ForschProj},
pages = {39--46},
title = {{Supermon: A high-speed cluster monitoring system}},
volume = {2002-Janua},
year = {2002}
}
@article{Hoffman2005,
author = {Hoffman, John J. and Byrd, Andrew and Mohror, Kathryn M. and Karavanic, Karen L.},
doi = {10.1109/IPDPS.2005.350},
file = {:home/tinker/Studium/10-Semester/Analyse eines Forschungthemas/Paper/pperfgrid.pdf:pdf},
isbn = {0769523129},
journal = {Proc. - 19th IEEE Int. Parallel Distrib. Process. Symp. IPDPS 2005},
mendeley-groups = {ForschProj},
title = {{PPerfGrid: A grid services-based tool for the exchange of heterogeneous parallel performance data}},
volume = {2005},
year = {2005}
}
@article{Prodan2002,
author = {Prodan, R. and Fahringer, T.},
doi = {10.1109/CLUSTR.2002.1137723},
file = {:home/tinker/Studium/10-Semester/Analyse eines Forschungthemas/Paper/ZENTURIO.pdf:pdf},
isbn = {0769517455},
issn = {15525244},
journal = {Proc. - IEEE Int. Conf. Clust. Comput. ICCC},
keywords = {Application software,Automatic control,Automatic generation control,Computer architecture,Grid computing,Instruments,Mesh generation,Performance analysis,Portals,Software testing},
mendeley-groups = {ForschProj},
pages = {9--18},
title = {{ZENTURIO: An experiment management system for cluster and Grid computing}},
volume = {2002-Janua},
year = {2002}
}
@article{Agelastos2014,
abstract = {Understanding how resources of High Performance Compute platforms are utilized by applications both individually and as a composite is key to application and platform performance. Typical system monitoring tools do not provide sufficient fidelity while application profiling tools do not capture the complex interplay between applications competing for shared resources. To gain new insights, monitoring tools must run continuously, system wide, at frequencies appropriate to the metrics of interest while having minimal impact on application performance. We introduce the Lightweight Distributed Metric Service for scalable, lightweight monitoring of large scale computing systems and applications. We describe issues and constraints guiding deployment in Sandia National Laboratories' capacity computing environment and on the National Center for Supercomputing Applications' Blue Waters platform including motivations, metrics of choice, and requirements relating to the scale and specialized nature of Blue Waters. We address monitoring overhead and impact on application performance and provide illustrative profiling results.},
author = {Agelastos, Anthony and Allan, Benjamin and Brandt, Jim and Cassella, Paul and Enos, Jeremy and Fullop, Joshi and Gentile, Ann and Monk, Steve and Naksinehaboon, Nichamon and Ogden, Jeff and Rajan, Mahesh and Showerman, Michael and Stevenson, Joel and Taerat, Narate and Tucker, Tom},
doi = {10.1109/SC.2014.18},
file = {:home/tinker/Studium/10-Semester/Analyse eines Forschungthemas/Paper/The Lightweight Distributed Metric Service$\backslash$: A Scalable Infrastructure for Continuous.pdf:pdf},
isbn = {978-1-4799-5500-8},
issn = {21674337},
journal = {Int. Conf. High Perform. Comput. Networking, Storage Anal. SC},
keywords = {resource management,resource monitoring},
mendeley-groups = {ForschProj},
number = {January},
pages = {154--165},
title = {{The Lightweight Distributed Metric Service: A Scalable Infrastructure for Continuous Monitoring of Large Scale Computing Systems and Applications}},
volume = {2015-Janua},
year = {2014}
}
@article{Liang1999,
abstract = {In this paper, we describe the ClusterProbe, an open, flexible,$\backslash$nscalable, and Java-based cluster monitoring tool. The tool provides an$\backslash$nopen environment by developing a multiple-protocol communication$\backslash$ninterface that can be connected to various types of external accesses$\backslash$nfrom the clients. ClusterProbe is flexible that the monitoring tool can$\backslash$nbe easily extended to adapt to the resource changes by using the$\backslash$nJava-RMI mechanism. In addition, the design of ClusterProbe allows it to$\backslash$nscale up to commodious capacity with its cascading hierarchical$\backslash$narchitecture. Several useful services are implemented based on$\backslash$nClusterProbe, including the visualization of cluster resource$\backslash$ninformation in various form and cluster fault management. The tool has$\backslash$nbeen used to assist the execution of a cluster-based search engine and a$\backslash$ndistributed N-body application. All experiments demonstrate high$\backslash$nefficiency},
author = {Liang, Zhengyu Liang Zhengyu and Sun, Yundong Sun Yundong and Wang, Cho-Li Wang Cho-Li},
doi = {10.1109/IWCC.1999.810895},
file = {:home/tinker/Studium/10-Semester/Analyse eines Forschungthemas/Paper/ClusterProbe.pdf:pdf},
isbn = {0-7695-0343-8},
journal = {ICWC 99. IEEE Comput. Soc. Int. Work. Clust. Comput.},
mendeley-groups = {ForschProj},
title = {{ClusterProbe: an open, flexible and scalable cluster monitoring$\backslash$ntool}},
year = {1999}
}
@article{Desai2008,
abstract = {In this paper, we describe disparity, a tool that does parallel, scalable anomaly detection for clusters. Disparity uses basic statistical methods and scalable reduction operations to perform data reduction on client nodes and uses these results to locate node anomalies. We discuss the implementation of disparity and present results of its use on a SiCortex SC5832 system.},
author = {Desai, Narayan and Bradshaw, Rick and Lusk, Ewing},
doi = {10.1109/ICPP-W.2008.30},
file = {:home/tinker/Studium/10-Semester/Analyse eines Forschungthemas/Paper/disparity.pdf:pdf},
isbn = {9780769533759},
issn = {15302016},
journal = {Proc. Int. Conf. Parallel Process. Work.},
mendeley-groups = {ForschProj},
pages = {116--120},
title = {{Disparity: Scalable anomaly detection for clusters}},
year = {2008}
}
@article{Kluge2012,
abstract = {The task of performance analysis and optimization grows more and more challenging with the increasing scale and complexity of large computing systems. The need for a holistic system analysis becomes apparent when traditional approaches do not collect the information that is required to investigate performance penalties caused by shared system resources. We have developed a distributed approach that is able to collect and process performance data from shared system resources. We call our software implementation of this approach Dataheap and have integrated it with a traditional program tracing facility. In this paper we describe the needs that have driven this development as well as connections to related projects. Dataheap is based on a threaded server, distributed agents that collect performance data, a storage backend that makes use of different databases, and access libraries that allow external systems to retrieve current and historic performance data. The server subsequently processes incoming performance data and allows to create secondary metrics on the fly which helps to transform individual system characteristics to standard performance metrics. Finally, we briefly illustrate how this approach has enhanced our performance debugging capabilities as well as our research on energy efficient computing. ?? 2012 Published by Elsevier Ltd.},
author = {Kluge, Michael and Hackenberg, Daniel and Nagel, Wolfgang E.},
doi = {10.1016/j.procs.2012.04.215},
file = {:home/tinker/Studium/10-Semester/Analyse eines Forschungthemas/Paper/dataheap.pdf:pdf},
issn = {18770509},
journal = {Procedia Comput. Sci.},
keywords = {Distributed systems,Performance analysis tools},
mendeley-groups = {ForschProj},
pages = {1969--1978},
publisher = {Elsevier Masson SAS},
title = {{Collecting distributed performance data with dataheap: Generating and exploiting a holistic system view}},
url = {http://dx.doi.org/10.1016/j.procs.2012.04.215},
volume = {9},
year = {2012}
}
@article{Mooney2004,
abstract = {We present NWPerf, a new system for analyzing fine granularity performance metric data on large-scale supercomputing clusters. This tool is able to measure application efficiency on a system wide basis from both a global system perspective as well as providing a detailed view of individual applications. NWPerf provides this service while minimizing the impact on the performance of user applications. We describe the type of information that can be derived from the system, and demonstrate how the system was used detect and eliminate a performance problem in an application application that improved performance by up to several thousand percent. The NWPerf architecture has proven to be a stable and scalable platform for gathering performance data on a large 1954-CPU production Linux cluster at PNNL.},
author = {Mooney, Ryan and Schmidt, Kenneth P. and Studham, R. Scott and Nieplocha, Jarek},
doi = {10.1109/CLUSTR.2004.1392637},
file = {:home/tinker/Studium/10-Semester/Analyse eines Forschungthemas/Paper/nwperf.pdf:pdf},
isbn = {0780386949},
issn = {15525244},
journal = {Proc. - IEEE Int. Conf. Clust. Comput. ICCC},
mendeley-groups = {ForschProj},
pages = {379--389},
title = {{NWPerf: A system wide performance monitoring tool for large Linux clusters}},
year = {2004}
}
@article{Massie2004,
abstract = {Ganglia is a scalable distributed monitoring system for high performance computing systems such as clusters and Grids. It is based on a hierarchical design targeted at federations of clusters. It relies on a multicast-based listen/announce protocol to monitor state within clusters and uses a tree of point-to-point connections amongst representative cluster nodes to federate clusters and aggregate their state. It leverages widely used technologies such as XML for data representation, XDR for compact, portable data transport, and RRDtool for data storage and visualization. It uses carefully engineered data structures and algorithms to achieve very low per-node overheads and high concurrency. The implementation is robust, has been ported to an extensive set of operating systems and processor architectures, and is currently in use on over 500 clusters around the world. This paper presents the design, implementation, and evaluation of Ganglia along with experience gained through real world deployments on systems of widely varying scale, configurations, and target application domains over the last two and a half years. {\textcopyright} 2004 Elsevier B.V. All rights reserved.},
author = {Massie, Matthew L. and Chun, Brent N. and Culler, David E.},
doi = {10.1016/j.parco.2004.04.001},
file = {:home/tinker/Studium/10-Semester/Analyse eines Forschungthemas/Paper/ganglia.pdf:pdf},
isbn = {1510495304},
issn = {01678191},
journal = {Parallel Comput.},
keywords = {Clusters,Distributed systems,Monitoring},
mendeley-groups = {ForschProj},
number = {7},
pages = {817--840},
title = {{The ganglia distributed monitoring system: Design, implementation, and experience}},
volume = {30},
year = {2004}
}
@article{Huck2005,
abstract = { Parallel applications running on high-end computer systems manifest a complexity of performance phenomena. Tools to observe parallel performance attempt to capture these phenomena in measurement datasets rich with information relating multiple performance metrics to execution dynamics and parameters specific to the application-system experiment. However, the potential size of datasets and the need to assimilate results from multiple experiments makes it a daunting challenge to not only process the information, but discover and understand performance insights. In this paper, we present PerfExplorer, a framework for parallel performance data mining and knowledge discovery. The framework architecture enables the development and integration of data mining operations that will be applied to large-scale parallel performance profiles. PerfExplorer operates as a client-server system and is built on a robust parallel performance database (PerfDMF) to access the parallel profiles and save its analysis results. Examples are given demonstrating these techniques for performance analysis of ASCI applications.},
author = {Huck, K.a. and a.D. Malony},
doi = {10.1109/SC.2005.55},
file = {:home/tinker/Studium/10-Semester/Analyse eines Forschungthemas/Paper/PerfExplorer.pdf:pdf},
isbn = {1-59593-061-2},
journal = {ACM/IEEE SC 2005 Conf.},
mendeley-groups = {ForschProj},
title = {{PerfExplorer: A Performance Data Mining Framework For Large-Scale Parallel Computing}},
year = {2005}
}
@article{Worringen2005,
author = {Worringen, Joachim},
doi = {10.1109/CLUSTR.2005.347052},
file = {:home/tinker/Studium/10-Semester/Analyse eines Forschungthemas/Paper/perfbase-preprint{\_}ieee-cluster2005.pdf:pdf},
isbn = {0780394852},
issn = {15525244},
journal = {Proc. - IEEE Int. Conf. Clust. Comput. ICCC},
keywords = {Experiment management,Performance analysis,Python,SQL,XML},
mendeley-groups = {ForschProj},
pages = {1--18},
title = {{Experiment management and analysis with perfbase}},
year = {2005}
}
@article{,
file = {:media/Daten/Studium/10-Semester/Analyse eines Forschungthemas/Paper/HPE Cluster Test - Admin Guide.pdf:pdf},
mendeley-groups = {ForschProj},
number = {November},
title = {{HPE Cluster Test Administration Guide}},
year = {2015}
}
@article{Quintero2014,
author = {Quintero, Dino},
file = {:media/Daten/Studium/10-Semester/Analyse eines Forschungthemas/Paper/IBM High Performance Computing Cluster Health Check.pdf:pdf},
keywords = {AIX AIX 5L BladeCenter developerWorks eServer Glob},
mendeley-groups = {ForschProj},
pages = {124},
title = {{IBM High Performance Computing Cluster Health Check}},
year = {2014}
}
@article{Burtscher2010,
abstract = {HPC systems are notorious for operating at a small fraction of their peak performance, and the ongoing migration to multi-core and multi-socket compute nodes further complicates performance optimization. The readily available performance evaluation tools require considerable effort to learn and utilize. Hence, most HPC application writers do not use them. As remedy, we have developed PerfExpert, a tool that combines a simple user interface with a sophisticated analysis engine to detect probable core, socket, and node-level performance bottlenecks in each important procedure and loop of an application. For each bottle-neck, PerfExpert provides a concise performance assessment and suggests steps that can be taken by the programmer to improve performance. These steps include compiler switches and optimization strategies with code examples. We have applied PerfExpert to several HPC production codes on the Ranger supercomputer. In all cases, it correctly identified the critical code sections and provided accurate assessments of their performance.},
author = {Burtscher, Martin and Kim, Byoung Do and Diamond, Jeff and McCalpin, John and Koesterke, Lars and Browne, James},
doi = {10.1109/SC.2010.41},
file = {:media/Daten/Studium/10-Semester/Analyse eines Forschungthemas/Paper/PerfExpert.pdf:pdf},
isbn = {9781424475575},
journal = {2010 ACM/IEEE Int. Conf. High Perform. Comput. Networking, Storage Anal. SC 2010},
keywords = {Bottleneck diagnosis,HPC systems,Multicore performance,Performance analysis,Performance metric},
mendeley-groups = {ForschProj},
title = {{PerfExpert: An easy-to-use performance diagnosis tool for HPC applications}},
year = {2010}
}

